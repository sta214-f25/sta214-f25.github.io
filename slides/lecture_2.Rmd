---
author: "Ciaran Evans"
title: Parametric models and logistic regression
output:
  xaringan::moon_reader:
    css: "lab-slides.css"
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

### Warmup activity

.large[
Work on the activity (handout) with a neighbor, then we will discuss as a class.
]

---

### Warmup

.large[
$$\text{odds} = \dfrac{\pi}{1 - \pi}$$

If $\pi = 0.2$, calculate the odds.
]

---

### Warmup

.large[
$$\text{odds} = \dfrac{\pi}{1 - \pi}$$

What happens to odds as $\pi \to 0$? As $\pi \to 1$?
]

---

### Warmup

.large[
$$\text{odds} = \dfrac{\pi}{1 - \pi}$$
]

```{r setup, include=F}
library(tidyverse)
library(patchwork)
```

```{r echo=F, message=F, fig.width=7, fig.height=5, fig.align='center'}
p <- seq(0, 0.99, 0.01)
odds <- p/(1-p)
data.frame(p, odds) |>
  ggplot(aes(x = p, y = odds)) +
  geom_line() +
  labs(x = "Probability", y = "Odds") +
  theme_bw(base_size = 16) +
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
```

---

### Warmup

.large[
$$\text{log-odds} = \log (\text{odds}) = \log \left( \dfrac{\pi}{1 - \pi} \right)$$

What happens to log-odds as $\pi \to 0$? As $\pi \to 1$?
]

---

### Warmup

.large[
$$\text{log-odds} = \log (\text{odds}) = \log \left( \dfrac{\pi}{1 - \pi} \right)$$
]

```{r echo=F, message=F, fig.width=9, fig.height=5, fig.align='center'}
p <- seq(0.01, 0.99, 0.01)
odds <- p/(1-p)
p1 <- data.frame(p, odds) |>
  ggplot(aes(x = p, y = odds)) +
  geom_line() +
  labs(x = "probability", y = "odds") +
  theme_bw(base_size = 16) +
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank())

p2 <- data.frame(p, odds) |>
  ggplot(aes(x = p, y = log(odds))) +
  geom_line() +
  labs(x = "probability", y = "log-dds") +
  theme_bw(base_size = 16) +
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank())

p1 + p2
```

---

### Last time

.large[
**Don't fit linear regression with a binary response**
]

```{r, echo=F, fig.width=10, fig.height=5, fig.align='center', message=F, warning=F}
dengue <- read.csv("https://sta279-s22.github.io/labs/dengue.csv")

p1 <- dengue %>%
  ggplot(aes(x = Age, y = Dengue)) +
  geom_point(size = 2) +
  geom_smooth(method="lm", se=F, lwd=1.5) +
  theme_bw() +
  theme(text = element_text(size = 20))

p2 <- dengue %>%
  ggplot(aes(x = WBC, y = Dengue)) +
  geom_point(size = 2) +
  geom_smooth(method="lm", se=F, lwd=1.5) +
  theme_bw() +
  theme(text = element_text(size = 20))

p1 + p2
```

---

### Revisiting the linear regression model

.large[
$$Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i$$
]

```{r echo=F, message=F, fig.width=6, fig.height=5, fig.align='center'}
x <- seq(0, 1, 0.01)
y <- 1 + x
data.frame(x, y) |>
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(x = "X", y = "Y") +
  theme_classic(base_size = 16) +
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
```

.large[
.question[
Will all of the observations fall exactly on the line?
]
]

---

### Revisiting the linear regression model

.large[
$$Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i$$
]

```{r echo=F, message=F, fig.width=6, fig.height=5, fig.align='center'}
x <- seq(0, 1, 0.01)
y <- 1 + x
data.frame(x, y) |>
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(x = "X", y = "Y") +
  theme_classic(base_size = 16) +
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
```

.large[
.question[
Given a value of $X$, how do I know where the values of Y are likely to be?
]
]

---

### Revisiting the linear regression model

.large[
$$Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i$$
]

```{r echo=F, message=F, fig.width=6, fig.height=5, fig.align='center'}
x <- seq(0, 1, 0.005)
y <- 1 + x + rnorm(length(x), sd=0.25)
data.frame(x, y) |>
  ggplot(aes(x = x, y = y)) +
  geom_abline(slope=1, intercept=1) +
  geom_point() +
  labs(x = "X", y = "Y") +
  theme_classic(base_size = 16) +
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
```

.large[
.question[
What do we often assume about the distribution of $\varepsilon$?
]
]

---

### Revisiting the linear regression model

---

### Parametric modeling

.large[

A regression model is an example of a more general process called **parametric modeling**

* **Step 1:** Choose a reasonable distribution for $Y_i$
* **Step 2:** Build a model for the parameters of interest
* **Step 3:** Fit the model

]

---

### Step 1: Choose a reasonable distribution for $Y_i$

.large[
.question[
What do I mean by a *distribution*?
]
]

---

### Step 1: Choose a reasonable distribution for $Y_i$

.large[
.question[
What do I mean by a *distribution*?
]
]

.large[
* A **distribution** tells us what outcomes are possible for $Y_i$, and how often these outcomes occur.

Here the possible values of $Y_i$ are 0 (no dengue) and 1 (dengue).

.question[
How often do these values occur in the population?
]

]

---

### Step 1: Choose a reasonable distribution for $Y_i$

.large[
.question[
What do I mean by a *distribution*?
]
]

.large[
* A **distribution** tells us what outcomes are possible for $Y_i$, and how often these outcomes occur.

Here the possible values of $Y_i$ are 0 (no dengue) and 1 (dengue).

.question[
How often do these values occur in the population?
]

]

.large[
* We don't know, so we will estimate from the sample
* We assume the probability $Y_i = 1$ depends on $Age_i$
]

---

### Step 1: Choose a reasonable distribution for $Y_i$

.large[
.question[
How should I describe the distribution of $Y_i$?
]
]

---

### Bernoulli distribution

.large[
**Definition:** Let $Y_i$ be a binary random variable, and $\pi_i = P(Y_i = 1)$. Then $Y_i \sim Bernoulli(\pi_i)$.

.question[
What do I mean by a *random variable*?
]
]

---

### Bernoulli distribution

.large[
**Definition:** Let $Y_i$ be a binary random variable, and $\pi_i = P(Y_i = 1)$. Then $Y_i \sim Bernoulli(\pi_i)$.

.question[
What do I mean by a *random variable*?
]
]

.large[
A **random variable** is an event that has a set of possible outcomes, but we don't know which one will occur

* Here $Y_i = 0$ or $1$
* Our goal is to use the observed data to estimate $\pi_i = P(Y_i = 1)$
]

---

### Second attempt at a model

.large[
$$Y_i \sim Bernoulli(\pi_i) \hspace{1cm} \pi_i = P(Y_i = 1 | Age_i)$$

$$\pi_i = \beta_0 + \beta_1 Age_i$$
]

.large[
.question[
Are there still any potential issues with this approach?
]
]

---

### Fixing the issues

---

### Logistic regression model

.large[

$Y_i =$ dengue status (0 = negative, 1 = positive) 

$Age_i =$ age (in years)

**Random component:** $\hspace{1cm} Y_i \sim Bernoulli(\pi_i)$

**Systematic component:** $\hspace{1cm} \log \left( \dfrac{\pi_i}{1 - \pi_i} \right) = \beta_0 + \beta_1 \ Age_i$

]

---

### Logistic regression model

.large[

$Y_i =$ dengue status (0 = negative, 1 = positive) 

$Age_i =$ age (in years)

**Random component:** $\hspace{1cm} Y_i \sim Bernoulli(\pi_i)$

**Systematic component:** $\hspace{1cm} \log \left( \dfrac{\pi_i}{1 - \pi_i} \right) = \beta_0 + \beta_1 \ Age_i$

.question[
Why is there no noise term $\varepsilon_i$ in the logistic regression model? Discuss for 1--2 minutes with your neighbor, then we will discuss as a class.
]

]

---

### Fitting the logistic regression model

.large[
$$\hspace{1cm} Y_i \sim Bernoulli(\pi_i)$$
$$\hspace{1cm} \log \left( \dfrac{\pi_i}{1 - \pi_i} \right) = \beta_0 + \beta_1 \ Age_i$$

```{r, include=F}
library(knitr)
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
   lines <- options$output.lines
   if (is.null(lines)) {
     return(hook_output(x, options))  # pass to default hook
   }
   x <- unlist(strsplit(x, "\n"))
   more <- "..."
   if (length(lines)==1) {        # first n lines
     if (length(x) > lines) {
       # truncate the output, but add ....
       x <- c(head(x, lines), more)
     }
   } else {
     x <- c(more, x[lines], more)
   }
   # paste these lines together
   x <- paste(c(x, ""), collapse = "\n")
   hook_output(x, options)
 })
```

```{r, eval=F}
m1 <- glm(Dengue ~ Age, data = dengue, 
          family = binomial)
summary(m1)
```
]

---

### Fitting the logistic regression model

.large[
$$\hspace{1cm} Y_i \sim Bernoulli(\pi_i)$$
$$\hspace{1cm} \log \left( \dfrac{\pi_i}{1 - \pi_i} \right) = \beta_0 + \beta_1 \ Age_i$$

```{r, output.lines = 5:8}
m1 <- glm(Dengue ~ Age, data = dengue, 
          family = binomial)
summary(m1)
```
]

---

### Class activity

.large[
* Work with a neighbor on the class activity (handout)
* I will collect your work at the end of class

**For next time**, read sections 6.4 and 6.6 in the textbook
]

